---
title: "Final Group project"
author: "GUIDO BOZZANO, IGNACIO GAING AND JUAN ARANGUREN"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
  pdf_document:
    toc: yes
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(skimr)
library(kknn)
library(here)
library(tictoc)
library(vip)
library(ranger)
library(GGally)
```

# The problem: predicting credit card fraud

The goal of the project is to predict fraudulent credit card transactions.

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no?

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox <https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0> and save it in your `dsb` repo, under the `data` folder.

As we will be building a classifier model using tidymodels, there's two things we need to do:

1.  Define the outcome variable `is_fraud` as a factor, or categorical, variable, instead of the numerical 0-1 varaibles.
2.  In tidymodels, the first level is the event of interest. If we leave our data as is, `0` is the first level, but we want to find out when we actually did (`1`) have a fraudulent transaction

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv")) %>% 

  mutate(
    # in tidymodels, outcome should be a factor  
    is_fraud = factor(is_fraud),
    
    # first level is the event in tidymodels, so we need to reorder
    is_fraud = relevel(is_fraud, ref = "1")
         )

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

We also add some of the variables we considered in our EDA for this dataset during homework 2.

```{r}
# Begin the process of mutating our card_fraud dataframe.
card_fraud <- card_fraud %>% 

  # Create a series of new variables related to the transaction date/time.
  mutate( 
          # Extract the hour from the transaction datetime
          hour = hour(trans_date_trans_time),

          # Extract the weekday from the transaction datetime, labels provide the actual weekday name
          wday = wday(trans_date_trans_time, label = TRUE),

          # Extract the month name from the transaction datetime
          month_name = month(trans_date_trans_time, label = TRUE),

          # Calculate the age of the customer at the time of the transaction
          age = interval(dob, trans_date_trans_time) / years(1)
  ) %>% 
  
  # Rename the transaction year variable
  rename(year = trans_year) %>% 
  
  # Continue mutating by adding geographic calculations
  mutate(
    
    # Convert latitude/longitude to radians for both customer and merchant
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # Calculate the geographical distance between customer and merchant in miles using the Haversine formula
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + 
                                   cos(lat1_radians) * cos(lat2_radians) * 
                                   cos(long2_radians - long1_radians)),

    # Calculate the geographical distance between customer and merchant in kilometers using the Haversine formula
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + 
                                     cos(lat1_radians) * cos(lat2_radians) * 
                                     cos(long2_radians - long1_radians))
  )
```

## Exploratory Data Analysis (EDA)

You have done some EDA and you can pool together your group's expertise in which variables to use as features. You can reuse your EDA from earlier, but we expect at least a few visualisations and/or tables to explore teh dataset and identify any useful features.

Group all variables by type and examine each variable class by class. The dataset has the following types of variables:

1.  Strings
2.  Geospatial Data
3.  Dates
4.  Date/Times
5.  Numerical

Strings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.

***Strings to Factors***

-   `category`, Category of Merchant
-   `job`, Job of Credit Card Holder

***Strings to Geospatial Data***

We have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location.

-   `city`, City of Credit Card Holder
-   `state`, State of Credit Card Holder

## Exploring factors: how is the compactness of categories?

-   Do we have excessive number of categories? Do we want to combine some?

```{r}
# Count the number of occurrences of each category
# Calculate the proportion each category represents of the total
card_fraud_category <- card_fraud %>% 
  count(category, sort=TRUE) %>%  # Count occurrences for each category, sort in descending order
  mutate(perc = n/sum(n))         # Compute proportion of each category

# Display results
print(head(card_fraud_category, 5))

# Count the number of occurrences of each job
# Calculate the proportion each job represents of the total
card_fraud_job <- card_fraud %>% 
  count(job, sort=TRUE) %>%       # Count occurrences for each job, sort in descending order
  mutate(perc = n/sum(n))         # Compute proportion of each job

# Display first 5 rows of the results
print(head(card_fraud_job, 5))
```

The predictors `category` and `job` are transformed into factors.

```{r}
#| label: convert-strings-to-factors

card_fraud <- card_fraud %>% 
  mutate(category = factor(category),
         job = factor(job))

```

`category` has 14 unique values, and `job` has 494 unique values. The dataset is quite large, with over 670K records, so these variables don't have an excessive number of levels at first glance. However, it is worth seeing if we can compact the levels to a smaller number.

### Why do we care about the number of categories and whether they are "excessive"?

In some cases, we may encounter a dataset where certain categories contain only one record each. This situation poses a challenge because having such limited data makes it difficult to make accurate predictions using those categories as predictors for new data with the same labels. Additionally, if our modeling process involves generating dummy variables, a large number of categories can lead to an overwhelming number of predictors. This abundance of predictors can slow down the model fitting process. While it's acceptable if all the predictors are useful, it becomes problematic when we have categories with only one record since those predictors are not informative. Trimming them can significantly improve the speed and quality of the data fitting process.

If we possess subject matter expertise, we can manually combine categories based on our knowledge. This allows us to merge related categories and create a more meaningful representation of the data. However, if we lack the necessary expertise or performing manual category combination is time-consuming, we can use cutoffs based on the amount of data within each category as an alternative approach.

By setting cutoffs, we can determine if a category has enough data to be considered meaningful. If the majority of the data is concentrated in only a few categories, it may be reasonable for us to retain those categories and group the remaining ones into an "other" category. Another option could be to exclude the data points belonging to smaller categories altogether.

This strategy of using cutoffs and consolidating categories allows us to simplify the dataset and make the modeling process more efficient. By doing so, we improve our ability to derive meaningful insights from the data while maintaining the overall integrity of the analysis.

## Do all variables have sensible types?

In this project, we need to carefully consider each variable and decide what to do with it: should we keep it as is, transform it in some way, or drop it altogether? This process involves a mix of Exploratory Data Analysis and Feature Engineering. While exploring the data, it's often helpful to perform some simple feature engineering techniques to enhance our understanding and extract valuable insights.

Now, in this particular project, we have the luxury of having all the data available right from the beginning. This means that any transformations we apply can be performed on the entire dataset. It's worth noting that it's ideal to carry out these transformations using a recipe_step() within the tidymodels framework.

By encapsulating the transformations within a recipe_step(), we ensure that the same transformations are consistently applied to any new data that the recipe is used on as part of the modeling workflow. This approach helps prevent data leakage and reduces the chances of missing any crucial steps during the feature engineering process.

So, to summarize, the key idea is to carefully assess each variable, determine whether it needs to be kept, transformed, or dropped, and then perform the transformations using a recipe_step() within the tidymodels framework. This way, we maintain consistency, minimize the risk of data leakage, and ensure that the feature engineering process is properly executed throughout the modeling workflow.

## Which variables to keep in your model?

You have a number of variables and you have to decide which ones to use in your model. For instance, you have the latitude/lognitude of the customer, that of the merchant, the same data in radians, as well as the `distance_km` and `distance_miles`. Do you need them all?

From our exploratory data analysis (EDA), we found that not all variables in the dataset are necessary for creating an effective fraud prediction model. We've decided to focus on a select group of variables that provide significant insight based on our EDA.

The variables related to time (hour, wday, and month_name) have been selected because we identified patterns in fraudulent activities. We noted that fraudulent transactions tend to occur late at night, more frequently in the first half of the year, and are spread evenly across all days of the week.

We also chose the 'age' variable as our EDA indicated that younger people are more frequently affected by fraud than older people. The reasons for this could be multifaceted, such as the limited use of credit cards by older people or a higher propensity for younger people to fall victim to fraud schemes.

The dataset contains multiple distance-related variables, but to avoid redundancy in our model, we'll only use one - 'distance_km'. This is because multiple variables representing the same underlying information can lead to model overfitting and decreased performance.

We've decided to exclude the 'job' variable due to its high cardinality (494 unique values) and its limited predictive relevance for fraudulent transactions. However, the 'category' variable will be included, as our previous analysis showed significant differences in fraud rates between categories (e.g., online vs offline transactions).

Lastly, we will include the 'amount' and 'is_fraud' variables. The 'amount' variable was selected as fraudulent transactions often involve small sums, according to our EDA. The 'is_fraud' variable will serve as our target for prediction in the model.

Our choices are rooted in the objective to develop an effective and accurate fraud prediction model, based on relevant and impactful variables.


## Fit your workflows in smaller sample

You will be running a series of different models, along the lines of the California housing example we have seen in class. However, this dataset has 670K rows and if you try various models and run cross validation on them, your computer may slow down or crash.

Thus, we will work with a smaller sample of 10% of the values the original dataset to identify the best model, and once we have the best model we can use the full dataset to train- test our best model.

```{r}
# The objective is to select a smaller subset from the original 'card_fraud' dataframe.
# This can be useful for initial data exploration or model prototyping 
# where a full dataset might be computationally heavy.

# We'll use the 'slice_sample()' function from the 'dplyr' package 
# which allows us to randomly select a proportion of the rows.

my_card_fraud <- card_fraud %>% 
  # The 'prop = 0.10' argument specifies that we want 10% of the rows.
  # The selected rows are chosen randomly, and thus the sample should be representative if the data is randomly distributed.
  # Note: The actual number of rows in 'my_card_fraud' can be less than exact 10% of the original dataframe's rows
  # if 'card_fraud' does not have enough unique rows to fulfill the request.
  slice_sample(prop = 0.10) 

# Now 'my_card_fraud' contains a random subset that is approximately 10% the size of the original 'card_fraud' dataframe.
```

```{r}
# The objective here is to select specific columns from 'my_card_fraud' dataframe. 
# We use the 'select()' function from the 'dplyr' package for this task.

my_card_fraud <- my_card_fraud %>% 
  # The columns to be selected are 'is_fraud', 'amt', 'category', 'hour', 'wday', 'month_name', 'age', and 'distance_km'.
  # These columns have been chosen based on their potential relevance to the task at hand, which is presumably fraud prediction.
  select(is_fraud, amt, category, hour, wday, month_name, age, distance_km)

# Now 'my_card_fraud' only contains the selected columns.

# Display the first 5 rows of the 'my_card_fraud' dataframe using the 'head()' function.
print(head(my_card_fraud, 5))
```

## Split the data in training - testing

```{r}
# **Split the data**

set.seed(123)

data_split <- initial_split(my_card_fraud, # updated data
                           prop = 0.8, 
                           strata = is_fraud)

card_fraud_train <- training(data_split) 
card_fraud_test <- testing(data_split)
```

## Cross Validation

Start with 3 CV folds to quickly get an estimate for the best model and you can increase the number of folds to 5 or 10 later.

```{r}
set.seed(123)
cv_folds <- vfold_cv(data = card_fraud_train, 
                          v = 3, 
                          strata = is_fraud)
cv_folds 
```

## Define a tidymodels `recipe`

What steps are you going to add to your recipe? Do you need to do any log transformations?

```{r, define_recipe}
# Prepare a recipe for preprocessing the 'card_fraud_train' dataframe.
fraud_rec <- recipe(is_fraud ~ ., data = card_fraud_train) %>%
  
  # Apply log transformation to 'amt' to handle skewness.
  step_log(amt) %>% 
  
  # Handle new factor levels in the nominal variables before dummyfication.
  step_novel(all_nominal(), -all_outcomes()) %>% 
  
  # Convert all nominal variables into binary dummy variables.
  step_dummy(all_nominal(), -all_outcomes())

# Display the defined preprocessing recipe.
print(fraud_rec)
```

Once you have your recipe, you can check the pre-processed dataframe

```{r}
# Apply the defined preprocessing recipe to the training data.
prepped_data <- 
  fraud_rec %>% # Use the preprocessing recipe object defined earlier
  prep() %>% # Execute the preprocessing steps defined in the recipe on the training data
  juice() # Extract the preprocessed dataframe from the recipe

# Use the 'glimpse()' function to get a concise overview of the preprocessed dataframe
glimpse(prepped_data)
```

## Define various models

You should define the following classification models:

1.  Logistic regression, using the `glm` engine
2.  Decision tree, using the `C5.0` engine
3.  Random Forest, using the `ranger` engine and setting `importance = "impurity"`)\
4.  A boosted tree using Extreme Gradient Boosting, and the `xgboost` engine
5.  A k-nearest neighbours, using 4 nearest_neighbors and the `kknn` engine

```{r, define_models}
# Model Building

# 1. Choose a `model type`
# 2. Set the `engine`
# 3. Set the `mode`:  classification

# Logistic regression
log_spec <-  logistic_reg() %>%  # Model type
  set_engine(engine = "glm") %>%  # Model engine
  set_mode("classification") # Model mode

# Show the model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # Adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 
```

## Bundle recipe and model with `workflows`

```{r, define_workflows}
# Bundle recipe and model with `workflows`

# Logistic Regression workflow
log_wflow <- 
  workflow() %>% # Create new workflow
  add_recipe(fraud_rec) %>%   # Add preprocessing recipe
  add_model(log_spec)   # Add logistic regression model

# Decision Tree workflow
tree_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(tree_spec) 

# Random Forest workflow
rf_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(rf_spec) 

# XGBoost workflow
xgb_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(xgb_spec)

# k-NN workflow
knn_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(knn_spec)
```

## Fit models

You may want to compare the time it takes to fit each model. `tic()` starts a simple timer and `toc()` stops it

```{r, fit_models}
# List of workflows
wflows <- list(log_wflow, tree_wflow, rf_wflow, xgb_wflow, knn_wflow)
# Corresponding names
wflow_names <- c("logistic regression", "decision tree", "random forest", "xgboost", "k-NN")

# Empty lists to store results and metrics
res_list <- list()
metrics_list <- list()
time_list <- list()

# For each workflow
for (i in 1:length(wflows)) {
  tic()
  wflow_res <- wflows[[i]] %>% 
    fit_resamples(
      resamples = cv_folds, 
      metrics = metric_set(recall, precision, f_meas, accuracy, kap, roc_auc, sens, spec),
      control = control_resamples(save_pred = TRUE)
    )
  time <- toc()
  time_list[[i]] <- time[[4]]
  cat(paste(wflow_names[i], "takes", time[[4]], "seconds to run.\n"))
  
  # Save results and metrics
  res_list[[i]] <- wflow_res
  metrics_list[[i]] <- wflow_res %>% collect_metrics(summarize = TRUE)
}

```

## Compare models

```{r, compare_models}
# Combine metrics
model_compare <- bind_rows(metrics_list) %>% 
  mutate(model = rep(wflow_names, each = 8),  # Update '8' if you change the number of metrics
         time = rep(time_list, each = 8), 
         time = str_sub(time, end = -13) %>% as.double())

# Filter and arrange the accuracy rates
accuracy_rates <- model_compare %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean))

# Print the top two models with their accuracy rates
cat("The top two models based on accuracy are:\n")
cat(paste(accuracy_rates$model[1], "with an accuracy of", scales::percent(accuracy_rates$mean[1], accuracy = 0.01), "\n"))
cat(paste(accuracy_rates$model[2], "with an accuracy of", scales::percent(accuracy_rates$mean[2], accuracy = 0.01), "\n"))
```

## Which metric to use

This is a highly imbalanced data set, as roughly 99.5% of all transactions are ok, and it's only 0.5% of transactions that are fraudulent. A `naive` model, which classifies everything as ok and not-fraud, would have an accuracy of 99.5%, but what about the sensitivity, specificity, the AUC, etc?

## `last_fit()`

```{r}
# Perform last fit on XGBoost model using the test set
last_fit_xgb <- last_fit(xgb_wflow, 
                         split = data_split,
                         metrics = metric_set(
                           accuracy, f_meas, kap, precision,
                           recall, roc_auc, sens, spec))

# Collect and summarize metrics from last fit on test set
last_fit_xgb_metrics <- last_fit_xgb %>% collect_metrics(summarize = TRUE)

# Print metrics for last fit on test set
cat("Metrics for last fit on test set:\n")
print(last_fit_xgb_metrics)

# Fit XGBoost model on the training set using fit_resamples()
xgb_fit_res <- xgb_wflow %>% 
  fit_resamples(
    resamples = cv_folds,
    metrics = metric_set(
      accuracy, f_meas, kap, precision,
      recall, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  )

# Collect and summarize metrics for XGBoost model on the training set
xgb_fit_metrics <- xgb_fit_res %>% collect_metrics(summarize = TRUE)

# Print metrics for XGBoost model on training set
cat("Metrics for XGBoost model on training set:\n")
print(xgb_fit_metrics)
```

## Get variable importance using `vip` package

```{r}
library(vip)

# Generate the feature importance chart
last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10) +
  theme_light()

#From this chart we can observe that the amount variable has the greatest level of importance with 38%, followed by the category_grocery_pos with 20% and hour with 10%. Last, age with 0.9%
```

## Plot Final Confusion matrix and ROC curve

```{r}
## Final Confusion Matrix

last_fit_xgb %>%
  collect_predictions() %>% 
  conf_mat(is_fraud, .pred_class) %>% 
  autoplot(type = "heatmap")


## Final ROC curve
last_fit_xgb %>% 
  collect_predictions() %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot()
```

## Calculating the cost of fraud to the company

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms. Compare your model vs the naive classification that we do not have any fraudulent transactions.

```{r}
#| label: savings-for-cc-company
# Fit the model and obtain predictions
best_model_preds <- xgb_wflow %>% 
  fit(data = card_fraud_train) %>%  
  augment(new_data = card_fraud)

# Compute confusion matrix
conf_matrix <- best_model_preds %>% 
  conf_mat(truth = is_fraud, estimate = .pred_class)

# Select relevant columns for cost analysis
cost <- best_model_preds %>%
  select(is_fraud, amt, pred = .pred_class) 

# Calculate different cost components
cost <- cost %>%
  mutate(
    false_naives = ifelse(is_fraud == 1, amt, 0),                  # Naive false: all transactions considered not fraud
    false_negatives = ifelse(pred == 0 & is_fraud == 1, amt, 0),   # False negatives: predicted not fraud, but actually fraud
    false_positives = ifelse(pred == 1 & is_fraud == 0, amt, 0),   # False positives: predicted fraud, but actually not fraud
    true_positives = ifelse(pred == 1 & is_fraud == 1, amt, 0),    # True positives: predicted fraud and actually fraud
    true_negatives = ifelse(pred == 0 & is_fraud == 0, amt, 0)     # True negatives: predicted not fraud and actually not fraud
  )

# Summarize the cost components
cost_summary <- cost %>% 
  summarise(across(starts_with(c("false", "true", "amt")), ~ sum(.x, na.rm = TRUE)))

cost_summary <- cost_summary / 1000000  # Convert cost values to millions

cost_summary

# Analyze cost summary
false_negatives_cost <- cost_summary$false_negatives
false_naives_cost <- cost_summary$false_naives

# Compare cost components
if (false_negatives_cost < false_naives_cost) {
  message("False negatives are costing the company less in terms of money.")
} else if (false_naives_cost < false_negatives_cost) {
  message("False naives are costing the company less in terms of money.")
} else {
  message("False negatives and false naives are costing the company the same amount.")
}

# Calculate the total savings in fraud refunds
total_savings <- false_naives_cost - false_negatives_cost
message("With our model, we are saving $", total_savings, " million in fraud refunds.")
```

-   If we use a naive classifier thinking that all transactions are legitimate and not fraudulent, the cost to the company is `r scales::dollar(cost_summary$false_naives)`.

-   With our best model, the total cost of false negatives, namely transactions our classifier thinks are legitimate but which turned out to be fraud, is `r scales::dollar(cost_summary$false_negatives)`.

-   Our classifier also has some false positives, `r scales::dollar(cost_summary$false_positives)`, namely flagging transactions as fraudulent, but which were legitimate. Assuming the card company makes around 2% for each transaction (source: <https://startups.co.uk/payment-processing/credit-card-processing-fees/>), the amount of money lost due to these false positives is `r scales::dollar(cost_summary$false_positives * 0.02)`

-   The \$ improvement over the naive policy is `r scales::dollar(cost_summary$false_naives - cost_summary$false_negatives - cost_summary$false_positives * 0.02)`.
